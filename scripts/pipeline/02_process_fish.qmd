---
title: "Fish Surveys"
date: today
format: 
  html:
    theme: minty
    self-contained: true
    code-fold: true
    toc: true 
    toc-depth: 3
    toc-location: right
    html-table-processing: none
execute:
  fig-width: 10
---

```{r setup, message = F, warning = F, fig.width = 10, fig.height = 10, echo = F}
options(scipen = 999)

# Hook to format inline numeric expressions with comma separators:
knitr::knit_hooks$set(inline = function(x) {
  if (!is.numeric(x)) {
    # For non-numeric values, just return as character
    return(as.character(x))
  }
  # Format numbers with comma as big.mark, no scientific notation
  format(x, big.mark = ",", scientific = FALSE)
})

library(PristineSeasR)
library(sf)
library(hms)
library(readxl)
library(janitor)
library(lubridate)
library(gt)
library(pointblank)
library(tidyverse)
library(bigrquery)
library(leaflet)
library(leaflet.extras)

ps_paths <- PristineSeasR::get_sci_drive_paths()

exp_id <- "PNG_2024"

exp_path <- file.path(ps_paths$expeditions, str_replace(exp_id, "_", "-"))

bigrquery::bq_auth(email = "marine.data.science@ngs.org")

bq_connection <- DBI::dbConnect(bigrquery::bigquery(), project = "pristine-seas")
```

```{r}
# Helper functions
fish_station_fields <- c("ps_site_id",  "method", "ps_station_id", "depth_strata", "avg_depth", 
                         "bottom_type", "diver", "n_transects", "surveyed_area")

# Assign a depth stratum label based on numeric depth. 

assign_depth_stratum <- function(avg_depth_m, 
                                 shallow_threshold       = 14,
                                 super_shallow_threshold = 6) {
  case_when(avg_depth_m == 0 ~ "surface",
            avg_depth_m <= super_shallow_threshold ~ "supershallow",
            avg_depth_m <= shallow_threshold ~ "shallow",
            avg_depth_m <= 30 ~ "deep",
            TRUE ~ NA_character_)}

#' Mapping from depth stratum labels to suffix strings (nominal depths).
#' Useful for constructing the station ID convention (e.g. "05m").

stratum_to_suffix <- c("surface"      = "00m",
                       "supershallow" = "05m",
                       "shallow"      = "10m",
                       "deep"         = "20m")

# A safer function for retrieving WM record data for a single AphiaID

safe_validate_aphiaID <- function(id) {
  
  tryCatch({
    rec <- worrms::wm_record(id)
    
    tibble(current_aphia_id = id,
           rank             = rec$rank,
           status           = rec$status, 
           valid_name       = rec$valid_name,
           valid_aphia_id   = rec$valid_AphiaID) 
  },
  error = function(e) {
    message(glue::glue("❌ Failed for AphiaID: {id}"))
    tibble(current_aphia_id = id,
           rank = NA_character_,
           status = "not found",
           valid_aphia_id = NA_character_, 
           valid_name = NA_character_)}
  )
}

# 
get_taxonomic_ranks <- function(id) {
  tryCatch({
    worrms::wm_classification(id) |>
      select(rank, scientificname) |>
      pivot_wider(names_from = rank, values_from = scientificname) |>
      mutate(accepted_aphia_id = id)
  }, error = function(e) {
    tibble(accepted_aphia_id = id)
  })
}
```

This script is meant to streamline the entire fish data pipeline—from ingesting and cleaning raw CSVs, performing taxonomy lookups, and flagging QA/QC issues, to computing station-level summaries and loading everything into our database in BigQuery. This ensures consistent, high-quality fish survey data ready for analysis

## Stations

```{r}
fish_lut <- read_csv(file.path(ps_paths$projects, "prj-legacy-db/data/processed/taxa/taxa_luts.uvs_fish_pacific.csv"),
                     show_col_types = FALSE) 

# -- 1. Read supporting site data --------------------------------------------

sites <- read_rds(file.path(exp_path, "data/primary/processed/sites.rds"))

uvs_sites <- sites$uvs

# -- 2. Diver files list -----------------------------------------------------

diver_files <- list(AMF = "PNG_2024_fish_fieldbook_AMF.xlsx",
                    ALG = "PNG_2024_fish_fieldbook_ALG_FINAL.xlsx",
                    VAS = "PNG_2024_fish_fieldbook_VAS.xlsx",
                    JEC = "PNG_2024_fish_fieldbook_JEC_FINAL.xlsx",
                    AJM = "PNG_2024_fish_fieldbook_AJKM_Final_clean.xlsx")

# -- 3. Read and clean raw observation files ---------------------------------

raw_obs <- map_dfr(diver_files, 
        
        function(file) {
          
          full_path <- file.path(exp_path, "data", "primary", "raw", "fish", file)
          
          read_xlsx(full_path,
                            sheet = "observations") |>
            clean_names() |>
            rename(accepted_name = taxon_valid_name,
                   ps_site_id    = ps_station_id,
                   station_label = depth_strata,
                   count         = n_ind) |>
            filter(!is.na(ps_site_id)) |> 
            mutate(ps_site_id    = str_replace(ps_site_id, "(\\d+)$", ~ str_pad(.x, 3, pad = "0")),
                   depth_m       = as.numeric(depth_m),
                   station_label = str_to_lower(station_label),
                   obs_id = paste("PNG_2024_fish", diver, str_pad(row_number(), 3, pad = "0"), sep = "_"))
          }
        ) |>  
  mutate(terminal_phase = if_else(is.na(is_tp), FALSE, TRUE)) |> 
  select(obs_id, ps_site_id, diver, station_label, depth_m, transect, taxon_code, accepted_name, count, min_tl_cm, max_tl_cm, terminal_phase)

# -- 4. Build station summary -----------------------------------------------

raw_stations <- raw_obs |>
  group_by(ps_site_id, diver, station_label) |>
  summarise(avg_depth_m    = round(mean(depth_m, na.rm = TRUE), 0),  # mean depth
            n_transects    = n_distinct(transect),         # distinct transects
            survey_area_m2 = n_transects * (25 * 4 + 25 * 2),  # Each transect = 25m x 4m wide (fish >= 20cm) or 2m (fish < 20cm)
            .groups = "drop" ) |>
  mutate(method         = "fish_blt",  # Standard method name for belt transects
         depth_strata   = assign_depth_stratum(avg_depth_m), # Convert numeric mean depth to a discrete stratum
         station_suffix = recode(depth_strata, !!!stratum_to_suffix), # Map stratum to nominal depth suffix for station ID
         ps_station_id  = paste0(ps_site_id, "_", station_suffix)) |>  # Full station ID construction
  # Reorder columns so ps_station_id is first
  select(ps_site_id, ps_station_id, method, depth_strata, avg_depth_m, diver, station_label, n_transects, survey_area_m2)

# Rejoin ps sttion id tot he obs table

raw_obs <- raw_obs |> 
  left_join(raw_stations |> 
              select(ps_station_id, ps_site_id, diver, station_label, depth_strata),
            by = c("ps_site_id", "diver", "station_label")) |> 
  relocate(ps_station_id, .after = ps_site_id) |> 
  relocate(depth_strata, .after = ps_station_id) 
```

### QAQC

The QAQC process for the stations table includes:

  - Checking for duplicate station IDs
    - Review and consolidate all transects across divers within the same site and depth strata (if needed). 
  - Flag stations whose depth strata does not match the relative station label given in the field
  - Flag stations that have more or less than the usual number of transects (3)

```{r}
library(pointblank)

create_agent(tbl = raw_stations, label = "Fish BLT stations QA/QC", tbl_name = "raw_stations") |> 
  rows_distinct(ps_station_id,
                label = "Station IDs are unique",
                actions = action_levels(stop_at = 0.001)) |> 
  # check if station label is the same as depth strata 
  col_vals_equal(columns = vars(station_label), 
                 value = vars(depth_strata),
                 label = "Station label and depth strata are the same",
                 actions = action_levels(warn = 0.001)) |>
  col_vals_equal(columns = vars(n_transects), 
                 value = 3,
                       label = "Number of transects per station is 3",
                       actions = action_levels(warn_at = 0.001)) |>
  interrogate() 
```

```{r}
# Review the duplicate stations

duped_station_ids <- raw_stations |> 
  get_dupes(ps_station_id)

# If appropriate, consolidate them

stations <- raw_stations |> 
  group_by(ps_site_id, ps_station_id, depth_strata) |> 
  summarize(divers = paste(unique(diver), collapse = "/"), 
            avg_depth_m = mean(avg_depth_m),
            n_transects = sum(n_transects),
            survey_area_m2 = sum(survey_area_m2),
            .groups = "drop") |> 
  arrange(ps_station_id)

# Check for any stations with not standard sampling effort

flagged <- stations |> 
    filter(n_transects < 2 | n_transects > 3)

if (nrow(flagged) > 0) {
  message("‼ ", nrow(flagged), " station(s) have less or more transects than usual")
  flagged |> 
          select(ps_station_id, depth_strata, divers, n_transects, survey_area_m2) |> 
          gt() |> 
          tab_options(table.font.size = 12,
                      data_row.padding = px(5),
                      table.width = pct(80))}

# Finally, join the uvs site info

stations <- stations |> 
  mutate(method = "fish_blt") |> 
  left_join(uvs_sites |> 
              select(exp_id, survey_type, ps_site_id, region, subregion, locality, habitat, exposure, date, time, notes), 
            by = "ps_site_id") |> 
  select(exp_id, survey_type, ps_site_id, ps_station_id, method, diver = divers, depth_m = avg_depth_m, depth_strata, 
         n_transects, area_m2 = survey_area_m2, date, time, region, subregion, locality, habitat, exposure, notes) |> 
  mutate(exposure = if_else(is.na(exposure), "unknown", exposure),
         habitat = if_else(is.na(habitat), "unknown", habitat)) |> 
  mutate(diver = case_when(diver == "ALG" ~ "Allison Green",
                           diver == "AMF" ~ "Alan Friedlander",
                           diver == "VAS" ~ "Vyvyan Summers",
                           diver == "JEC" ~ "Jenn Caselle",
                           diver == "AJKM" ~ "Andrew McKinnis",
                           diver == "ALG/AMF" ~ "Allison Green/Alan Friedlander",
                           diver == "ALG/VAS" ~ "Allison Green/Vyvyan Summers"))
```

### Summary of work

Overall, we conducted fish surveys at **`r n_distinct(stations$ps_site_id)`** sites and **`r n_distinct(stations$ps_station_id)`** stations during the expedition. Across the **`r n_distinct(stations$region)`** regions sampled, we surveyed a total area of **`r sum(stations$area_m2)`** m2 of reef habitats (@tbl-fish-stations).
 
```{r eval = T, include = T}
#| label: tbl-fish-stations
#| tbl-cap: "Number of fish survey stations by habitat and depth strata"

stations |> 
  group_by(region, habitat, depth_strata) |>
  summarise(n = n_distinct(ps_station_id), .groups = "drop") |> 
  pivot_wider(names_from = habitat, values_from = n, values_fill = 0) |> 
  mutate(Total = rowSums(across(where(is.numeric)), na.rm = TRUE)) |>
  gt(groupname_col = "region") |> 
  tab_options(table.font.size = 12,
              data_row.padding = px(5),
              table.width = pct(90),
              row_group.as_column = T) |> 
  tab_source_note(source_note = "Depth stratum: supershallow (< 6 m), shallow (7 - 15 m), and deep (>= 15 m).") |> 
  tab_spanner(label = "Reef Habitats", 
              columns = c("fore reef", "back reef", "channel", "patch reef")) |> 
  cols_label(depth_strata = "Depth Strata")
```

::: {.panel-tabset}

##### By Habitat

```{r}
# Habitat map
habitat_pal <- c("fore reef"  = "#C5FFFD",
                 "back reef"  = "#DF2935",
                 "channel"    = "#D8CC34",
                 "patch reef" = "#772D8B")

stations_sf <- stations |>
  left_join(uvs_sites |> 
              select(ps_site_id, latitude, longitude),
            by = "ps_site_id") |>
  mutate(habitat = factor(habitat, levels = names(habitat_pal))) |>
  group_by(ps_site_id, region, subregion, locality, habitat, exposure, latitude, longitude) |> 
  summarize(team = paste(unique(diver), collapse = "/"),
            strata = paste(unique(paste(depth_strata, " (",depth_m ,"m)", sep = "")), 
                            collapse = "\n"),
            surveyed_area = sum(area_m2),
            n_stations = n_distinct(ps_station_id),
            .groups = "drop") |>
  sf::st_as_sf(coords = c("longitude", "latitude"), crs = 4326) 

mapview::mapview(stations_sf,
                 zcol = "habitat",
                 legend = TRUE,
                 col.regions = habitat_pal,
                 map.types = "Esri.WorldImagery",
                 layer.name = "Habitat",
                 popup = leafpop::popupTable(stations_sf, 
                                             zcol = c("ps_site_id", "strata", "team", "surveyed_area", "habitat", "exposure"))) |> 
  leafem::addMouseCoordinates() |> 
  addFullscreenControl()
```

##### By Exposure

```{r}
# Exposure map
exposure_pal <- c("leeward"  = "#1f78b4",
                  "windward" = "#33a02c",
                  "lagoon"   = "#ff7f00")

stations_sf <- stations_sf |> 
  mutate(exposure = factor(exposure, levels = names(exposure_pal)))

mapview::mapview(stations_sf,
                 zcol = "exposure",
                 legend = TRUE,
                 col.regions = exposure_pal,
                 map.types = "Esri.WorldImagery",
                 layer.name = "Habitat",
                 popup = leafpop::popupTable(stations_sf, zcol = c("ps_site_id", "strata", "team", "surveyed_area","habitat", "exposure"))) |> 
  leafem::addMouseCoordinates() |> 
  addFullscreenControl()
```

:::

## Observations

### QA/QC

The QAQC process for the observations table includes:

  - Checking for missing values in key columns (station ID, transect, taxon code, taxon, count, lengths)
  - Check that in min length < max length
  - Check that all species are in the taxonomy look up table
  - Check for length over max reported length in LUT

```{r message = F}
raw_obs <- raw_obs |> 
  rename(taxon = accepted_name) |> 
  mutate(taxon = case_when(taxon_code == "ACANTH_JUV" ~ "Acanthuridae",
                                   taxon_code == "SCARID_JUV" ~ "Scaridae",
                                   taxon_code == "PARAPERCIS_SPP_PIC" ~ "Parapercis",
                                   taxon_code == "HALICHOERES SPP PIC" ~ "Halichoeres",
                                   taxon_code == "LABRID_SPP" ~ "Labridae",
                                   TRUE ~ taxon),
         taxon = bdc::bdc_clean_names(taxon)$names_clean)

create_agent(tbl = raw_obs, label = "Fish BLT Observations QA/QC", tbl_name = "raw_obs") |> 
  col_vals_not_null(columns = vars(ps_station_id, transect, taxon, count),
                label = "Station ID, diver, transect, taxon, count are not null",
                actions = action_levels(warn_at = 0.001)) |>
  col_vals_lte(columns = vars(min_tl_cm),
                value = vars(max_tl_cm),
                label = "Min length is less than max length",
                actions = action_levels(warn_at = 0.001), na_pass = TRUE) |>
  interrogate() 
```

```{r}
raw_obs <-  raw_obs |> 
  mutate(avg_length = rowMeans(across(c(x=min_tl_cm, max_tl_cm)), na.rm = TRUE)) |> 
  select(-min_tl_cm, -max_tl_cm) |> 
  relocate(avg_length, .after = count)

observed_taxa <- raw_obs |> 
  count(taxon_code, taxon)

name_to_accepted_AphiaID <- fish_lut |>
    pivot_longer(cols = c("taxon", "accepted_name"), names_to = "orig_col", values_to = "lookup_name") |>
    filter(!is.na(lookup_name)) |>
    distinct(accepted_aphia_id, lookup_name)

observed_taxa <- observed_taxa |> 
  left_join(name_to_accepted_AphiaID, by = c("taxon" = "lookup_name")) |> 
  left_join(fish_lut |> 
              distinct(accepted_aphia_id, accepted_name, rank, tl_max, lw_a, lw_b, ltl_ratio, trophic_group, family),
            by = c("accepted_aphia_id" = "accepted_aphia_id"))

observed_taxa |> 
  filter(is.na(trophic_group)) |> 
  distinct(taxon, accepted_name, rank, family, accepted_aphia_id) |> 
  arrange(family) |> 
  filter(!is.na(accepted_name)) |> 
  write_csv(file.path(exp_path, "data", "primary", "raw", "fish", "missing_trophic_groups.csv"))

new_taxa <- observed_taxa |> 
  filter(is.na(accepted_aphia_id), !is.na(taxon)) |> 
  distinct(taxon)
```

```{r}
# 1. Query WoRMS via 'gna_verifier()' from {taxize}, for new_taxa$taxon_clean
new_taxa_qaqc <- taxize::gna_verifier(new_taxa$taxon, data_sources = c(9)) |>
  select(taxon            = submittedName, 
         aphia_id         = recordId,
         name_status      = taxonomicStatus,
         current_name     = currentCanonicalSimple, 
         current_aphia_id = currentRecordId) |>
  mutate(name_status      = str_to_lower(name_status),
         aphia_id         = as.integer(str_extract(aphia_id, "\\d+")),
         current_aphia_id = as.integer(str_extract(current_aphia_id, "\\d+")))

# 2. Gather worms records for all current_aphia_id in new_taxa_qaqc
worms_records <- map_dfr(new_taxa_qaqc$current_aphia_id, safe_validate_aphiaID)

# 3. Merge WoRMS data into new_taxa_qaqc and finalize columns
new_taxa_qaqc <- new_taxa_qaqc |>
  left_join(worms_records, by = "current_aphia_id") |> 
  mutate(rank        = str_to_lower(rank),
         name_status = if_else(name_status == "accepted", status, name_status),
         taxon_code  = case_when(rank == "species" ~ str_c(str_sub(word(taxon, 1), 1, 2), ".", str_sub(word(taxon, 2), 1, 4)),
                                 rank == "genus"   ~ str_c(str_sub(taxon, 1, 4), ".SP"),
                                 rank == "family"  ~ str_c(str_sub(taxon, 1, 4), ".SPP"),
                                 TRUE ~ NA_character_)) |> 
  rename(accepted_name     = valid_name, 
         accepted_aphia_id = valid_aphia_id) |> 
  select(-status, -current_name, -current_aphia_id) |> 
  mutate(taxon_code = str_to_upper(taxon_code)) |>
  select(taxon_code, everything())

# Split into new synonyms and new taxa

new_synonyms <- new_taxa_qaqc |> 
  filter(accepted_aphia_id %in% fish_lut$accepted_aphia_id) |> 
  left_join(fish_lut |> 
              select(accepted_aphia_id:last_col()),
            by = c("accepted_aphia_id" = "accepted_aphia_id")) 

# New taxa

new_taxa <- new_taxa_qaqc |> 
  filter(!accepted_aphia_id %in% fish_lut$accepted_aphia_id)

## Taxonomy for new taxa

new_taxonomy <- map_dfr(.x = new_taxa$accepted_aphia_id, 
                        .f = get_taxonomic_ranks) |>
  clean_names() |>
  select(accepted_aphia_id, kingdom, phylum, class, order, family, genus)

new_taxa <- new_taxa |> 
  left_join(new_taxonomy, by = "accepted_aphia_id") 

## Common names

common_fams <- fish_lut |> 
  distinct(family, common_family) 

new_common_names_fb <- rfishbase::species(new_taxa$accepted_name) |> 
  select(accepted_name = Species, spec_code = SpecCode, common_name = FBname)

new_taxa <- new_taxa |> 
  left_join(new_common_names_fb) |> 
  left_join(common_fams) |> 
  mutate(common_name = coalesce(common_name, common_family))

##  LW params, habitat, ecology from FishBase

new_fishbase_params <- rfishbase::estimate(new_taxa$accepted_name) |> 
  clean_names() |> 
  select(spec_code      = spec_code, 
         tl_max         = max_length_tl,  # Maximum Total Length (cm)
         trophic_lvl    = troph,          # Trophic level
         trophic_lvl_se = se_troph,       # Standard error of TL
         feeding_path   = feeding_path,
         lw_a           = a,
         lw_b           = b) |> 
  mutate(tl_max_source = "FishBase",
         ltl_ratio     = 1,
         lw_type       = "TL",
         lw_source     = "FishBase",
         trophic_group = NA_character_)

new_taxa <- new_taxa |> 
  left_join(new_fishbase_params, by = "spec_code")

# Habitat, Ecology, and fishery use

new_ecology_info <- rfishbase::ecology(new_taxa$accepted_name) |> 
  clean_names() |> 
  select(spec_code, feeding_type, diet = herbivory2)

new_hab_and_fishery_info <- rfishbase::species(new_taxa$accepted_name) |>
  clean_names() |>
  select(spec_code, habitat_zone = demers_pelag, fisheries_use = importance) |>
  mutate(fisheries_use = if_else(fisheries_use == " ", NA_character_, fisheries_use))

new_taxa <- new_taxa |> 
  left_join(new_hab_and_fishery_info) |> 
  left_join(new_ecology_info)

master_lut <- bind_rows(fish_lut, new_taxa, new_synonyms)
```

```{r}
name_to_accepted_AphiaID <- master_lut |>
    pivot_longer(cols = c("taxon", "accepted_name"), names_to = "orig_col", values_to = "lookup_name") |>
    filter(!is.na(lookup_name)) |>
    distinct(accepted_aphia_id, lookup_name)

obs <- raw_obs |> 
  left_join(name_to_accepted_AphiaID, 
            by = c("taxon" = "lookup_name")) |> 
  left_join(master_lut |> 
              distinct(accepted_aphia_id, accepted_name, rank, tl_max, lw_a, lw_b, ltl_ratio, trophic_group, family, common_family, order))

## Large fish

# obs |>
#   filter(avg_length > tl_max) |>
#   mutate(abs_diff = avg_length - tl_max,
#          pct_diff = round(100*(abs_diff)/tl_max)) |>
#   select(obs_id, ps_site_id, accepted_name, depth_strata, diver, transect, avg_length, tl_max, pct_diff, abs_diff) |>
#   filter(!diver %in% c("AMF", "AJKM"),
#          abs_diff > 5) |>
#   arrange(desc(pct_diff)) 
#   write_csv(file.path(exp_path, "data", "primary", "raw", "fish", "length_over_max.csv"))
# 
# ## Small fish
# 
# obs |>
#   filter(avg_length < 0.05*tl_max)
```

# Biomass

```{r}
# Calculate Fish Biomass per observation 

biomass_per_obs <- obs |> 
  mutate(# Calculate average length
         abs_diff = if_else(!is.na(tl_max), avg_length - tl_max, NA_real_),
         pct_diff = if_else(!is.na(abs_diff), round(100 * abs_diff / tl_max), NA_real_),
         ## Only adjust avg_length downward if tl_max is available and the diff is large
         avg_length = case_when(!is.na(tl_max) & abs_diff >= 5 ~ tl_max,
                                TRUE ~ avg_length),
         # Biomass calculations
         avg_biomass   = lw_a * (avg_length * ltl_ratio)^lw_b,
         total_biomass = count * avg_biomass,
         # Size-dependent transect area
         area_m2    = if_else(avg_length >= 20, 100, 50),
         biomass_m2 = total_biomass / area_m2,
         count_m2     = count / area_m2,
         # Round biomass fields
         across(c(avg_biomass, total_biomass, biomass_m2), round, 4)) |> 
  select(obs_id, ps_site_id, ps_station_id, station_label, depth_strata, depth_m, diver, transect, accepted_aphia_id, accepted_name, rank,
         count, avg_length, avg_biomass, total_biomass, biomass_m2, count_m2,
         order, family, common_family, trophic_group, 
         everything(), -taxon, -taxon_code, -tl_max, -lw_a, -lw_b, -ltl_ratio, -area_m2, -abs_diff, -pct_diff) 

# Summarize observations to transects 

biomass_by_transect_and_taxa <- biomass_per_obs |> 
  group_by(ps_site_id, ps_station_id, station_label, depth_strata, diver, transect, 
           accepted_aphia_id, accepted_name, order, family, common_family, trophic_group) |> 
  summarize(sum_length_count = sum(avg_length * count, na.rm = TRUE),
            across(c(count, total_biomass, biomass_m2, count_m2),  sum, na.rm = TRUE), 
            .groups = "drop")

# Summarize transects to stations

biomass_by_station_and_taxa <- biomass_by_transect_and_taxa |>
  # 1. Complete all transect × taxon combinations (within each station)
  complete(nesting(ps_site_id, ps_station_id, diver, depth_strata, station_label, transect),
           nesting(accepted_aphia_id, accepted_name, order, family, common_family, trophic_group),
           fill = list(sum_length_count = 0,
                       count            = 0,
                       biomass_m2       = 0,
                       count_m2         = 0,
                       total_biomass    = 0)) |>
  # 2. Group to station level (across transects)
  group_by(ps_site_id, ps_station_id, depth_strata,
           accepted_aphia_id, accepted_name, order, family, common_family, trophic_group) |>
  summarise(total_count       = sum(count),
            avg_length_wm     = ifelse(sum(count) > 0, sum(sum_length_count) / sum(count), NA),
            avg_count         = mean(count),
            avg_count_m2      = mean(count_m2),
            avg_biomass_m2    = mean(biomass_m2),
            .groups = "drop")

biomass_by_transect <- biomass_by_transect_and_taxa |> 
  group_by(ps_site_id, ps_station_id, station_label, depth_strata, diver, transect) |> 
  summarize(n_taxa = n_distinct(accepted_name),
            across(c(count, total_biomass, biomass_m2, count_m2), .fns = sum, na.rm = TRUE), 
            .groups = "drop")

avgs_by_station <- biomass_by_transect |> 
  group_by(ps_site_id, ps_station_id, depth_strata) |> 
  summarize(n_transects = n_distinct(transect),
            across(c(n_taxa, count, total_biomass, biomass_m2, count_m2), .fns = mean, na.rm = TRUE), 
                        .groups = "drop")
```

# Export

```{r}
# write multiple object for later

save(stations, biomass_per_obs, biomass_by_station_and_taxa, 
     file = file.path(exp_path, "data", "primary", "processed", "fish", "fish_staging.RData"))
```

